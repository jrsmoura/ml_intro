{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "- Pode ser usado tanto para regressão, quanto para classificação, ambas para os casos linear e não-linear\n",
    "- Um dos mais populares modelos de ML\n",
    "\n",
    "\n",
    "#### O que veremos\n",
    "\n",
    "- O que é o SVM\n",
    "- Classificação linear com SVM\n",
    "- Classificação não-linear com SVM\n",
    "\n",
    "\n",
    "----\n",
    "## O que é o SVM\n",
    "\n",
    " Vamos entender como o SVM funciona olhando para a imagem abaixo.\n",
    "\n",
    "![img_01](https://jingwen-z.github.io/images/20190423-svm.png)\n",
    "\n",
    "É fácil concluir que ambas as classes podem ser separadas por uma linha (separação linear).\n",
    "\n",
    "O gráfico a esquerda representa as fronteiras de decisão dos três possíveis classificadores lineares.\n",
    "\n",
    "Um primeiro modelo gerou uma fronteira de decisão representada pela linha pontilhada, que claramente é um modelo muito ruim. Os outros dois modelos (roxo e vermelho) se sairam melhor, mas estão muito próximos um do outro, o que pode gerar uma má classificação para novos dados.\n",
    "\n",
    "Se olharmos para o gráfico da direita, a linha sólida (no meio) representa a fronteira de decisão gerada por um classificador SVM. Ela não só separa amabas as classes, como também dá espaço para novas instâncias serem bem classificadas.\n",
    "\n",
    "> Podemos pensar em classificadores SVM como o treinamento mais amplo (rua) possível entre as classes,s endo as instâncias localizadas no limite da rua seus *support vectors*.\n",
    "\n",
    "----\n",
    "\n",
    "## Classificação usando o LinearSVM\n",
    "\n",
    "Se fizermos a \"rua\" muito lagar, podemos acabar gerando um overfitting. Para equilibrar esta largura os modelos de SVC possuem um parâmetro chamado *soft margin classification* (figura abaixo). Se imposermos que todas as instâncias fiquem fora da rua pelo lado direito, estaremos fazendo uma classificação a força, o que fará com que o modelo não performe bem.\n",
    "\n",
    "![fig_02](https://img2018.cnblogs.com/blog/1012590/201903/1012590-20190331160549758-1258657163.png)\n",
    "\n",
    "Afim de evitar este problema, é preferível usar um modelo mais flexível. **Objetivo**: achar um bom balanço entre manter a rua mais larga o possível e limitar as violações de margem.  A este problema é que damos o nome de *soft margin classification*.\n",
    "\n",
    "\n",
    "No scikit-learn, este hiperparâmetro é denotado por `c`. Um valor pequeno de `c` leva a uma rua mais larga, mas gera um maior número de violações de margem (figura abaixo).\n",
    "\n",
    "![fig_03](https://img2018.cnblogs.com/blog/1012590/201903/1012590-20190331162727069-1127451993.png)\n",
    "\n",
    "Vamos exemplificar o uso do SVM para classificação Linear usando o dataset Iris para fazer uma classificação apenas para detectar flores do tipo Iris-Virginica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# obtendo os dados\n",
    "iris = datasets.load_iris()\n",
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = iris[\"data\"][:, (2, 3)]\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y = (iris[\"target\"]==2).astype(np.float64)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"LinearSVC\", LinearSVC(C=1, loss=\"hinge\", random_state=42)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "svm_clf.predict([[5.5, 1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Mesmo sendo um classificador eficiente, o `LinearSVC()` não irá resolver o problema de datasets não-lineares.\n",
    "\n",
    "Para este tipo de problema, podemos usar mais parâmetros para tentar fazer um melhor aprendizado. Aqui vamos ver o `PolynomialFeatures()` para aumentarmos o grau do nosso modelo.\n",
    "\n",
    "Uma revisãozinha (apenas em 2d $(x, y)$):\n",
    "\n",
    "Quando falamos de modelos lineares, estamos falando da equação da reta:\n",
    "\n",
    "$$\n",
    "    y = ax + b\n",
    "$$\n",
    "\n",
    "sendo $a$ e $b$ os parâmetros que queremos aprender.\n",
    "\n",
    "Ao adicionarmos o argumento `PolynomialFeatures()`, temos a possibilidade de aumentar o grau desta equação, por exemplo, para o segundo grau: `PolynomialFeatures(degree=2)`:\n",
    "\n",
    "$$\n",
    "    y = ax + bx^2 + c\n",
    "$$\n",
    "\n",
    "o que nos dá 3 parâmetros a serem aprendidos: $a, b, c$.\n",
    "\n",
    "No que se segue, vamos usar um polinômio de grau 3:\n",
    "$$\n",
    "    y = ax + bx^2 + cx^3 + d\n",
    "$$\n",
    "\n",
    "ou seja, 4 parâmetros: $a, b, c, d$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# imports específicos para este modelo\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "polyb_svm_clf = Pipeline([\n",
    "    ('poly_features', PolynomialFeatures(degree=3)),\n",
    "    ('scalar', StandardScaler()),\n",
    "    ('svm_clf', LinearSVC(C=10, loss='hinge', random_state=42))\n",
    "])\n",
    "\n",
    "polyb_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "![k-fold](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scores_pol = cross_val_score(polyb_svm_clf, X, y, cv=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scores_pol.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scores_svm = cross_val_score(svm_clf, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scores_svm.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A diferença entro os dois é no número de k-folds que precisamos para o polynomial pecisamos de `cv=23` para obtertmos uma convergência, enquanto que no caso linear, precisamos de apenas 5.\n",
    "\n",
    "Vamos dar uma olhada nas classificações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "C = 1.0  # SVM regularization parameter\n",
    "svc = svm.SVC(kernel='linear', C=C).fit(X, y)\n",
    "rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)\n",
    "poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)\n",
    "lin_svc = svm.LinearSVC(C=C).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we create an instance of SVM and fit out data. We do not scale our\n",
    "# data since we want to plot the support vectors\n",
    "# create a mesh to plot in\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# title for the plots\n",
    "titles = ['SVC with linear kernel',\n",
    "          'LinearSVC (linear kernel)',\n",
    "          'SVC with RBF kernel',\n",
    "          'SVC with polynomial (degree 3) kernel']\n",
    "\n",
    "\n",
    "for i, clf in enumerate((svc, lin_svc, rbf_svc, poly_svc)):\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "    plt.xlabel('Sepal length')\n",
    "    plt.ylabel('Sepal width')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(titles[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "-----\n",
    "\n",
    "## Árvores de Decisão\n",
    "\n",
    "- Assim como SVM, Árvores de Decisão (decision trees) são bem populares em ML e podem realizar tarefas tanto de classificação, quanto de regressão.\n",
    "- São as bases do Random Forest, um dos modelos mais poderosos que temos em ML hoje em dia.\n",
    "- Tem como vantagem de necessitar de muito pouca preparação dos dados.\n",
    "\n",
    "#### Treinamento e visualização de uma decision tree\n",
    "\n",
    "Vamos começar treinando uma decision tree simples para o dataset iris\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)]\n",
    "y = iris.target"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# importamos a biblioteca para visualização\n",
    "from sklearn.tree import export_graphviz"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "export_graphviz(\n",
    "    tree_clf,\n",
    "    out_file=\"iris_tree.dot\",\n",
    "    feature_names=iris.feature_names[2:],\n",
    "    class_names=iris.target_names,\n",
    "    rounded=True,\n",
    "    filled=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# para gerarmos uma imagem png, ou pdf, ou jpg .... a partir do *.dot, precisamos do graphviz\n",
    "# link para download: https://gitlab.com/api/v4/projects/4207231/packages/generic/graphviz-releases/6.0.1/windows_10_cmake_Release_graphviz-install-6.0.1-win64.exe\n",
    "!dot -Tpng ../imgs/iris_tree.dot -o ../imgs/iris.png\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pergunta: como fazemos previsões usando a árvore abaixo:\n",
    "\n",
    "![fig_04](../imgs/iris.png)\n",
    "\n",
    "    Começando da raiz da árvore (depth 0, top): petal length <= 2.45 cm.\n",
    "\n",
    "        | Se True => passamos para o nó filho da raiz, à esquerda (depth 1, left):\n",
    "        |     | Possui algum nó filho?\n",
    "        |     |      Se Não => Fim, a árvore prevê que a flor é da classe SETOSA\n",
    "        -----------------------------------------------------------------------------------\n",
    "        | Se False => passamos para o nó filho da raiz, à direita (depth 1, right):\n",
    "        |     | Peta width <= 1.75 cm:\n",
    "        |     |     Se Sim => passamos para o nó raiz, à esquerda (depth 2, left)\n",
    "        |     |         Possui algum nó filho?\n",
    "        |     |             Se Não => Fim, a árvore prevê que a flor é da classe VIRGINICA\n",
    "        |     -----------------------------------------------------------------------------\n",
    "        |     | Se não => passamos para o nó raiz, à direita (depth 2, rigth)\n",
    "        |     |     Possui algum nó filho?\n",
    "        |     |          Se Não => Fim, a árvore prevê que a flor é da classe VERISICOLOR\n",
    "\n",
    "-----\n",
    "\n",
    "O modelo já está treinado e já podemos usá-lo para realizar previsões."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tree_clf.predict_proba([[4.5, 2]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tree_clf.predict([[4.5, 2]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# cross-validation\n",
    "scores_tree = cross_val_score(tree_clf, X, y, cv=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores_tree.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Se olharmos para a árvore, veremos 3 informações adicionais:\n",
    "\n",
    "- Samples:\n",
    "\n",
    "    Valor de quantas amostras de treino o nó possui.\n",
    "\n",
    "- Values:\n",
    "\n",
    "    Nos diz quantas amostras de cada classe temos na amostra.\n",
    "\n",
    "\n",
    "- gini:\n",
    "\n",
    "    Medida de impureza de um nó. Um nó é dito puro quando todas as instâncias de treino pertencem a mesma classe. Exemplo: setosa é uma classe \"pura\"\n",
    "\n",
    "    $$\n",
    "        G(p) = \\sum_{k=1}^{N} p_k (1 - p_k) = 1 - \\sum_{k=1}^{K} p_{k}^{2}\n",
    "    $$\n",
    "\n",
    "Exemplo: Verisicolor => $G(p) = 1 - \\left( \\frac{0}{54} \\right)^2 - \\left( \\frac{49}{54} \\right)^2 - \\left( \\frac{5}{54} \\right)^2 \\approx 0.168$\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "### Regressão com Decision Tree"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# vamos usar um dataset sintético apenas para exemplificar o uso\n",
    "dataset = np.array(\n",
    "[[2,4],\n",
    " [3,9],\n",
    " [4,16],\n",
    " [5,25],\n",
    " [6,36],\n",
    " [8,64],\n",
    " [10,100],\n",
    " [12,144],\n",
    " [13,169]\n",
    "])\n",
    "X = dataset[:, 0:1].astype(int)\n",
    "y = dataset[:,1].astype(int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tree_rg = DecisionTreeRegressor(max_depth=5)\n",
    "tree_rg.fit(X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = plot_tree(tree_rg,\n",
    "            filled=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3451051fb9fa6a0fd2fb0c7157b0292bd597b42f654d61273dd0012a5a890db6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}